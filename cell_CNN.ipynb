{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSV7gNHJVi4A"
   },
   "source": [
    "## Sections:\n",
    "\n",
    "1.   Import packages\n",
    "2.   Load data\n",
    "3.   Data preprocessing\n",
    "\n",
    "    *   Standardization\n",
    "    *   Resizing\n",
    "\n",
    "4.   Data Augmentation\n",
    "\n",
    "    *   Rotate 180°\n",
    "  \n",
    "5.   Neural Network\n",
    "    \n",
    "    *   AlexNet\n",
    "    *   ResNet\n",
    "\n",
    "6.   Running the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHgWgCcWMkJM"
   },
   "source": [
    "# Import packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q3FoH3hNVhH2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random \n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import glob\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub \n",
    "from tensorflow import keras\n",
    "import time\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, ZeroPadding2D,\\\n",
    "     Flatten, BatchNormalization, AveragePooling2D, Dense, Activation, Add \n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ve4rjQqyjUsx"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0CLqV9GgEHjG"
   },
   "source": [
    "# Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8_SRY2RvqEAl"
   },
   "outputs": [],
   "source": [
    "def open_image(file_name):\n",
    "  #input:   String of the full path of the image that is to be loaded\n",
    "  #output:  Tensor (float32) of the loaded image\n",
    "    \n",
    "    return tf.convert_to_tensor(np.array(Image.open(file_name)), dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HOBRZ_s5ELSg"
   },
   "outputs": [],
   "source": [
    "def get_file_list(input_dir = '', test_predict = False):\n",
    "  #input:   String of the input dir which should contain the folders [Chondrocytes, Stemcells, Test]\n",
    "  #output:  A list of all the full paths of the images inside the folders\n",
    "  #info:    If test_predict = True the list of the test images is loaded and returned.\n",
    "  #         If not the \"normal\" file list of the Stemcell and Chondrocyte images is loaded and returned\n",
    "    \n",
    "    if test_predict:\n",
    "        file_list_test_predict = glob.glob(input_dir + 'Test//Test-both//*')\n",
    "        \n",
    "        return file_list_test_predict\n",
    "    \n",
    "    else:\n",
    "\n",
    "        file_list_chondrocytes = glob.glob(input_dir + 'Chondrocytes//*')\n",
    "        file_list_stemcells = glob.glob(input_dir + 'Stemcells//*')\n",
    "        file_list_all = file_list_chondrocytes\n",
    "        for f in file_list_stemcells:\n",
    "            file_list_all.append(f)\n",
    "\n",
    "        np.random.seed(13)\n",
    "        np.random.shuffle(file_list_all)\n",
    "\n",
    "        return file_list_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HoFSfpytqQa5"
   },
   "outputs": [],
   "source": [
    "def compute_is_stemcell(filename):\n",
    "  #input:   String of the full path of an image\n",
    "  #output:  Tensor containing the information if the image is a Stemcell (1) or Chondrocyte (0)\n",
    "\n",
    "    if 'Stemcell' in filename:\n",
    "        label = np.array([1])\n",
    "    else:\n",
    "        label = np.array([0])\n",
    "        \n",
    "    return tf.convert_to_tensor(label, dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TRn4UTBGVY6"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t8BMcXXwpWCj"
   },
   "outputs": [],
   "source": [
    "def data_preprocessing(data, data_standardization_switch = False, grayscale_switch = False, edge_detection_switch = False):\n",
    "  #input:   Tensor of the raw data of an image; Boolean information of which proprecessing methods are to be applied to the data\n",
    "  #output:  Tensor of the preprocessed data of an image\n",
    "  \n",
    "    if grayscale_switch:\n",
    "        data = rgb_to_grayscale(data)\n",
    "        \n",
    "    if edge_detection_switch:\n",
    "        data = edge_detection(data)\n",
    "        \n",
    "    if data_standardization_switch:\n",
    "        data = data_standardization(data)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y_GM87isbMve"
   },
   "outputs": [],
   "source": [
    "def edge_detection(data):\n",
    "  #input:   Tensor of the data of an image\n",
    "  #output:  Tensor of the edge detection data of an image\n",
    "    \n",
    "    return tf.convert_to_tensor(tf.image.sobel_edges(tf.convert_to_tensor(np.expand_dims(data.numpy(), axis=0)))[0,:,:,:,0], dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hE0dvoT0pW0l"
   },
   "outputs": [],
   "source": [
    "def data_standardization(data):\n",
    "  #input:   Tensor of the data of an image\n",
    "  #output:  Tensor of the standardized data of an image\n",
    "\n",
    "    return tf.convert_to_tensor(tf.image.per_image_standardization(data), dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UHrgdX0YpYnz"
   },
   "outputs": [],
   "source": [
    "def rgb_to_grayscale(data):\n",
    "  #input:   Tensor of the data of an image\n",
    "  #output:  Tensor of the grayscale data of an image\n",
    "    \n",
    "    rgb_data = tf.image.rgb_to_grayscale(data).numpy()[:,:,0]\n",
    "    rgb_3D = np.transpose(np.array((rgb_data, rgb_data, rgb_data)), (1,2,0))\n",
    "    return tf.convert_to_tensor(rgb_3D, dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AvmzeOcXqI8W"
   },
   "outputs": [],
   "source": [
    "def print_preprocessing_progress(f, number_of_files):\n",
    "  #input:   Integer of the current position in the \"for\" loop of going through the file list; Integer of the number of files inside the file list.\n",
    "  #output:  None\n",
    "  #info:    Prints the progress of the preprocessing (e.g. \"Picture 4/19 processed.\")\n",
    "  \n",
    "    print('Picture', str(f+1) + '/' + str(number_of_files),'processed.')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vvoHkjWPqK9k"
   },
   "outputs": [],
   "source": [
    "def resize_image(data, x_shape, y_shape):\n",
    "  #input:   Tensor of the data of an image; desired shape of the image (x and y)\n",
    "  #output:  Tensor of the resized data of an image\n",
    "    \n",
    "    return tf.convert_to_tensor(tf.image.resize(data, [x_shape, y_shape]), dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AU35ME_7qNyW"
   },
   "outputs": [],
   "source": [
    "def concatenate_data(dataset, image, label = None, augmented = False, test_predict = False):  \n",
    "  #input:   tf.data.Dataset of the already preprocessed images; Tensor of the data of an image(s); Booleans containing information of the current loop iteration.\n",
    "  #output:  tf.data.Dataset of the already preprocessed images and the currently as \"image\" passed image(s).\n",
    "    \n",
    "    if test_predict:\n",
    "        data = tf.data.Dataset.from_tensors((image.numpy()))\n",
    "\n",
    "        if not dataset:\n",
    "            dataset = data\n",
    "        else:\n",
    "            dataset = dataset.concatenate(data)\n",
    "            \n",
    "    else:\n",
    "    \n",
    "        if augmented:     \n",
    "\n",
    "            for i in range(tf.shape(image)[0]):\n",
    "                data = tf.data.Dataset.from_tensors((tf.convert_to_tensor(image, dtype = tf.float32).numpy()[i], \n",
    "                                                     tf.convert_to_tensor(label, dtype = tf.float32).numpy()[i]))\n",
    "\n",
    "                if not dataset:\n",
    "                    dataset = data\n",
    "                else:\n",
    "                    dataset = dataset.concatenate(data)\n",
    "\n",
    "        else:\n",
    "            data = tf.data.Dataset.from_tensors((image.numpy()[0], label.numpy()))\n",
    "\n",
    "            if not dataset:\n",
    "                dataset = data\n",
    "            else:\n",
    "                dataset = dataset.concatenate(data)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWiph-S3GbuB"
   },
   "source": [
    "## Data augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "me8RJqDnpctk"
   },
   "outputs": [],
   "source": [
    " def data_augmentation(data, label, progress,\n",
    "                      rotate_180_switch = False, mirroring_switch = False, zoom_switch = False,\n",
    "                      rotate_180_rate = 0.2, mirroring_rate = 0.2,\n",
    "                      zoom_rate = 0.2, zoom_factor = 2):\n",
    "  #input:   Tensor of the data of an image; Tensor of the label of this image; Float of the progress of the loop;\n",
    "  #         Booleans containing the information on which data augmentation methods are to be applied to the data\n",
    "  #         Floats contating the information of, on how much of the data the augmentation methods are to be applied.\n",
    "  #output:  Tensor containing the data of the original image and the augmented image(s);\n",
    "  #         Tensor containing the label of the original image and the augmented image(s).\n",
    "\n",
    "    data_new = []\n",
    "    label_new = []\n",
    "    data_new.append(data)\n",
    "    label_new.append(label)\n",
    "    augmented = False\n",
    "    \n",
    "    if rotate_180_switch and (progress <= rotate_180_rate):\n",
    "        data_rotated_180 = rotate_180(data)\n",
    "        data_new.append(data_rotated_180)\n",
    "        label_new.append(label)\n",
    "        augmented = True\n",
    "        \n",
    "    if mirroring_switch and (progress <= mirroring_rate):\n",
    "        data_mirrored = mirroring(data)\n",
    "        data_new.append(data_mirrored)\n",
    "        label_new.append(label)\n",
    "        augmented = True\n",
    "        \n",
    "    if zoom_switch and (progress <= zoom_rate):\n",
    "        data_zoomed = zoom(data, zoom_factor)\n",
    "        for i in range(tf.shape(data_zoomed)[0]):\n",
    "            data_new.append(data_zoomed[i])\n",
    "            label_new.append(label)\n",
    "        augmented = True\n",
    "            \n",
    "    return tf.convert_to_tensor(data_new, dtype = tf.float32), tf.convert_to_tensor(label_new, dtype = tf.float32), augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "14KqZzj4pfxd"
   },
   "outputs": [],
   "source": [
    "def zoom(data, zoom_factor = 2):\n",
    "  #input:   Tensor of the data of an image; zooming factor (e.g. =2 -> 4 images created; =4 -> 16 images created; =1.5 -> 1 image created)\n",
    "  #output:  Tensor of the data of the zoomed image(s)\n",
    "\n",
    "    x_dim = tf.shape(data)[0]\n",
    "    y_dim = tf.shape(data)[1]\n",
    "    data_zoomed = []\n",
    "    \n",
    "    for i in range(zoom_factor):\n",
    "        for j in range(zoom_factor):\n",
    "            snippet = data[int((i*x_dim/zoom_factor)):int((i+1)*x_dim/zoom_factor),\n",
    "                                             int((j*y_dim/zoom_factor)):int((j+1)*y_dim/zoom_factor)]\n",
    "            snippet_original_dims = tf.image.resize(snippet, (x_dim, y_dim))\n",
    "            data_zoomed.append(snippet_original_dims)\n",
    "\n",
    "    return tf.convert_to_tensor(data_zoomed, dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LmQiYaXuphbc"
   },
   "outputs": [],
   "source": [
    "def mirroring(data):\n",
    "  #input:   Tensor of the data of an image\n",
    "  #output:  Tensor of the data of a mirrored image\n",
    "    \n",
    "    return tf.convert_to_tensor(tf.image.flip_left_right(data), dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dDOxoxl2pjfW"
   },
   "outputs": [],
   "source": [
    "def rotate_180(data):\n",
    "  #input:   Tensor of the data of an image\n",
    "  #output:  Tensor of the data of a rotated (by 180°) image\n",
    "        \n",
    "    return tf.convert_to_tensor(tf.image.rot90(data, k=2), dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T7TTTqwMT6E3"
   },
   "outputs": [],
   "source": [
    "def graph_pict(data,is_stemcell):\n",
    "  #Displays a plot of 25 images and their labels.\n",
    "  #input: data=Tensorflow object, is_stemcell=Tensorflow object\n",
    "  \n",
    "    plt.figure(figsize=(10,10))\n",
    "    labels=tf.experimental.numpy.uint64(is_stemcell)\n",
    "    title=[]\n",
    "    for i in range(25):\n",
    "        ax = plt.subplot(5,5,i+1)\n",
    "        image=AUG_data[i]\n",
    "        #image=tf.squeeze(AUG_data[i], axis=-1)\n",
    "        plt.imshow(image)\n",
    "        title.append(str(labels[i]))\n",
    "        plt.title(title[i])\n",
    "        plt.axis(\"off\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0ce2xF4GhvG"
   },
   "source": [
    "# Neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o2HzMKKt24mM"
   },
   "outputs": [],
   "source": [
    "def runNet(data, aug_size, data_test_predict=[], epochs=30, batch_size=10, testsize=0.15, kfold=5, learn_rate=0.001, early_stopping_patience=20, shape=[227,227,3], ResNet=True, AlexNet=False):\n",
    "    # This function sets up the test and kfold training data. Additionaly some monitoring functions are added to stop overfitting and enable early stopping as well as saving the model.\n",
    "    # Afterwards in each loop iteration the model is trained with the set CNN-architecture and the respective training and validation data (kfold).\n",
    "    # In addition, after the training of the CNN, the test data is evaluated. Optionally, an additional test set can also be predicted.\n",
    "    \n",
    "    #Number of folds specified\n",
    "    num_folds = kfold\n",
    "    \n",
    "    #Get test size\n",
    "    test_size = int(aug_size*(np.ceil(testsize*len(data)/aug_size)))\n",
    "    \n",
    "    #Include callbacks\n",
    "    filepath='weights.best.hdf5'\n",
    "    checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max') # Callback to save the Keras model or model weights \n",
    "    earlystopper = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=early_stopping_patience, verbose=1,mode='min') # Early stopping function to prevent overfitting\n",
    "    callback = [checkpointer, earlystopper]\n",
    "\n",
    "    #Split in test and train data\n",
    "    test = data.take(test_size)\n",
    "    train = data.skip(test_size)\n",
    "    \n",
    "    #Get size\n",
    "    foldsize = int(len(train)/num_folds)\n",
    "    train_size = tf.data.experimental.cardinality(train).numpy()\n",
    "    test_size = tf.data.experimental.cardinality(test).numpy()\n",
    "    print(\"Training data size:\", int(train_size *(kfold-1)/kfold))\n",
    "    print(\"Test data size:\", test_size)\n",
    "    print(\"Validation data size:\", int(train_size /kfold))\n",
    "    \n",
    "    #Shuffle test data and divide into batches\n",
    "    test = test.shuffle(buffer_size = test_size, seed = 13)\n",
    "    test = test.batch(batch_size=batch_size, drop_remainder=True)\n",
    "    \n",
    "    #Iterate over folds\n",
    "    for fold in range(num_folds):\n",
    "        \n",
    "        print(\"\\n{}\".format(\"Fold number\"),fold+1)\n",
    "        \n",
    "        #Define model\n",
    "        if ResNet:\n",
    "            model = resnet50(shape)\n",
    "        if AlexNet:\n",
    "            model = alexNet()\n",
    "        #model.summary()\n",
    "        \n",
    "        #Compile model\n",
    "        model.compile(loss='binary_crossentropy', optimizer=tf.optimizers.SGD(lr=learn_rate), metrics=['accuracy'])\n",
    "        \n",
    "        #Split training data in training and validation data for current fold \n",
    "        valfold = train.take(foldsize*(fold+1))\n",
    "        valfold = valfold.skip(foldsize*fold)\n",
    "\n",
    "        trainfold1 = train.take(foldsize*fold)\n",
    "        trainfold2 = train.skip(foldsize*(fold+1))\n",
    "        trainfold = trainfold1.concatenate(trainfold2)\n",
    "\n",
    "        #Shuffle data  \n",
    "        trainfold = trainfold.shuffle(buffer_size = train_size, seed = 13)\n",
    "        valfold = valfold.shuffle(buffer_size = train_size, seed = 13)\n",
    "        \n",
    "        #Divide into batches\n",
    "        trainfold = trainfold.batch(batch_size=batch_size, drop_remainder=True)\n",
    "        valfold = valfold.batch(batch_size=batch_size, drop_remainder=True)\n",
    "\n",
    "        #Train network\n",
    "        model.fit(trainfold,\n",
    "                epochs=epochs,\n",
    "                validation_data=valfold,\n",
    "                validation_freq=1,\n",
    "                callbacks = callback)\n",
    "\n",
    "        #Evaluate test data\n",
    "        model.evaluate(test)\n",
    "        \n",
    "        #Print prediction of the additional test data \n",
    "        if not (len(data_test_predict) == 0):\n",
    "            prediction = model.predict(data_test_predict.batch(batch_size=1))\n",
    "            print(\"\\n{}\".format(\"Predictions for the additional test set:\"))\n",
    "            print(prediction)\n",
    "  \n",
    "    return model,test,train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-VF8X8cYKfnk"
   },
   "outputs": [],
   "source": [
    "def get_run_logdir(root_logdir):\n",
    "    #Input:   Directory where all TensorBoard files are stored\n",
    "    #Output:  The location of the exact directory that is named according to the current time the training phase starts\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5RzHWJK3nb-q"
   },
   "source": [
    "AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0R6i6Ic93drY"
   },
   "outputs": [],
   "source": [
    "def alexNet():\n",
    "    #This function defines the AlexNet model and returns it\n",
    "    \n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(227,227,3)),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "        keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "        keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(4096, activation='relu'),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(4096, activation='relu'),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UOMXLTVpKvAy"
   },
   "outputs": [],
   "source": [
    "def AlexNet(data, data_test_predict = [], epochs = 10, batch_size=10, train_split_rate = 0.7, learn_rate = 0.0001):\n",
    "    \n",
    "    # Split in train, test and validation\n",
    "    print(data)\n",
    "    a = int(len(data) * train_split_rate)\n",
    "    train = data.take(a)\n",
    "    data = data.skip(a)\n",
    "    b = int(len(data)/2)\n",
    "    test = data.take(b)\n",
    "    validation = data.skip(b)\n",
    "    \n",
    "    #Shuffle data  \n",
    "    train = train.shuffle(buffer_size = 100000, seed = 13)\n",
    "    test = test.shuffle(buffer_size = 100000, seed = 13)\n",
    "    validation = validation.shuffle(buffer_size = 100000, seed = 13)\n",
    "    \n",
    "    print(train)\n",
    "\n",
    "    #Get size\n",
    "    train_size = tf.data.experimental.cardinality(train).numpy()\n",
    "    test_size = tf.data.experimental.cardinality(test).numpy()\n",
    "    validation_size = tf.data.experimental.cardinality(validation).numpy()\n",
    "    print(\"Training data size:\", train_size)\n",
    "    print(\"Test data size:\", test_size)\n",
    "    print(\"Validation data size:\", validation_size)\n",
    "    \n",
    "    #Divide into batches\n",
    "    train = train.batch(batch_size=batch_size, drop_remainder=True)\n",
    "    test = test.batch(batch_size=1, drop_remainder=True)\n",
    "    validation = validation.batch(batch_size=1, drop_remainder=True)\n",
    "    \n",
    "    #Include callback TensorBoard\n",
    "    root_logdir = os.path.join(os.curdir, \"logs//fit//\")\n",
    "    run_logdir = get_run_logdir(root_logdir)\n",
    "    tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "    earlystopper = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, verbose=1,mode='min') # Early stopping function to prevent overfitting\n",
    "\n",
    "    \n",
    "    #Define model\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(227,227,3)),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "        keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "        keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(4096, activation='relu'),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(4096, activation='relu'),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    #Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=tf.optimizers.SGD(lr=learn_rate), metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    #Train network\n",
    "    model.fit(train,\n",
    "              epochs=epochs,\n",
    "              validation_data=validation,\n",
    "              validation_freq=1,\n",
    "              callbacks=[tensorboard_cb, earlystopper])\n",
    "    \n",
    "    #Evaluate test data\n",
    "    model.evaluate(test)\n",
    "    \n",
    "    if not (len(data_test_predict) == 0):\n",
    "        prediction = model.predict(data_test_predict.batch(batch_size=1))\n",
    "        print(prediction)\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3aNtzRsGndqV"
   },
   "source": [
    "ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QQbbOwBGnfVb"
   },
   "outputs": [],
   "source": [
    "def res_identity(x, filters):     \n",
    "    #resnet identity block  \n",
    "\n",
    "    x_skip = x # this will be used for addition with the residual block \n",
    "    f1, f2 = filters\n",
    "\n",
    "    #first block \n",
    "    x = Conv2D(f1, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_regularizer=l2(0.001))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activations.relu)(x)\n",
    "\n",
    "    #second block # bottleneck (but size kept same with padding)\n",
    "    x = Conv2D(f1, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(0.001))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activations.relu)(x)\n",
    "\n",
    "    # third block activation used after adding the input\n",
    "    x = Conv2D(f2, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_regularizer=l2(0.001))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    # x = Activation(activations.relu)(x)\n",
    "\n",
    "    # add the input \n",
    "    x = Add()([x, x_skip])\n",
    "    x = Activation(activations.relu)(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CFUCCryFnfYI"
   },
   "outputs": [],
   "source": [
    "def res_conv(x, s, filters):\n",
    "    # resnet convolutional building block\n",
    "\n",
    "    x_skip = x  # this will be used for addition with the residual block \n",
    "    f1, f2 = filters\n",
    "\n",
    "    # first block\n",
    "    x = Conv2D(f1, kernel_size=(1, 1), strides=(s, s), padding='valid', kernel_regularizer=l2(0.001))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activations.relu)(x)\n",
    "\n",
    "    # second block\n",
    "    x = Conv2D(f1, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(0.001))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activations.relu)(x)\n",
    "\n",
    "    #third block\n",
    "    x = Conv2D(f2, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_regularizer=l2(0.001))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # shortcut \n",
    "    x_skip = Conv2D(f2, kernel_size=(1, 1), strides=(s, s), padding='valid', kernel_regularizer=l2(0.001))(x_skip)\n",
    "    x_skip = BatchNormalization()(x_skip)\n",
    "\n",
    "    # add \n",
    "    x = Add()([x, x_skip])\n",
    "    x = Activation(activations.relu)(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nAZzTgO1nfbK"
   },
   "outputs": [],
   "source": [
    "def resnet50(shape = [227, 227, 3]):\n",
    "      \n",
    "    # construct resnet 50\n",
    "    input_im = Input(shape=(shape[0], shape[1], shape[2])) # image size\n",
    "    x = ZeroPadding2D(padding=(3, 3))(input_im)\n",
    "\n",
    "    # 1st stage\n",
    "    # maxpooling\n",
    "    x = Conv2D(64, kernel_size=(7, 7), strides=(2, 2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activations.relu)(x)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "    #2nd stage \n",
    "    # from here on only conv block and identity block\n",
    "    x = res_conv(x, s=1, filters=(64, 256))\n",
    "    x = res_identity(x, filters=(64, 256))\n",
    "    x = res_identity(x, filters=(64, 256))\n",
    "\n",
    "    # 3rd stage\n",
    "    x = res_conv(x, s=2, filters=(128, 512))\n",
    "    x = res_identity(x, filters=(128, 512))\n",
    "    x = res_identity(x, filters=(128, 512))\n",
    "    x = res_identity(x, filters=(128, 512))\n",
    "\n",
    "    # 4th stage\n",
    "    x = res_conv(x, s=2, filters=(256, 1024))\n",
    "    x = res_identity(x, filters=(256, 1024))\n",
    "    x = res_identity(x, filters=(256, 1024))\n",
    "    x = res_identity(x, filters=(256, 1024))\n",
    "    x = res_identity(x, filters=(256, 1024))\n",
    "    x = res_identity(x, filters=(256, 1024))\n",
    "\n",
    "    # 5th stage\n",
    "    x = res_conv(x, s=2, filters=(512, 2048))\n",
    "    x = res_identity(x, filters=(512, 2048))\n",
    "    x = res_identity(x, filters=(512, 2048))\n",
    "\n",
    "    # Average pooling and dense connection at the end\n",
    "    x = AveragePooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1, activation='sigmoid', kernel_initializer='he_normal')(x) #binary classification\n",
    "\n",
    "    # define the model \n",
    "    model = Model(inputs=input_im, outputs=x, name='Resnet50')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oi41hc6l_bjv"
   },
   "outputs": [],
   "source": [
    "def save_model(model, suffix=None):\n",
    "    \"\"\"\n",
    "    Saves a given model in a models directory and appends a suffix (str)\n",
    "    for clarity and reuse.\n",
    "    \"\"\"\n",
    "    # Create model directory with current time\n",
    "    modeldir = os.path.join(\"drive/My Drive/Data/models\",\n",
    "                            datetime.datetime.now().strftime(\"%Y%m%d-%H%M%s\"))\n",
    "    model_path = modeldir + \"-\" + suffix + \".h5\" # save format of model\n",
    "    print(f\"Saving model to: {model_path}...\")\n",
    "    model.save(model_path)\n",
    "    return model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "buwOVsI_lrYt"
   },
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    \"\"\"\n",
    "    Loads a saved model from a specified path.\n",
    "    \"\"\"\n",
    "    print(f\"Loading saved model from: {model_path}\")\n",
    "    model = tf.keras.models.load_model(model_path,\n",
    "                                        custom_objects={\"KerasLayer\":hub.KerasLayer})\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XE8pqTQYpyjM"
   },
   "outputs": [],
   "source": [
    "def unbatchify(data,test=False):\n",
    "    \"\"\"\n",
    "    Unbatch a data set and returns a numpy array of images and labels\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels =[]\n",
    "    #Loop to unbatch data\n",
    "    if not test:\n",
    "        for image,label in data.unbatch().as_numpy_iterator():\n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "        return images, labels\n",
    "    else:\n",
    "        for image,label in data.unbatch().as_numpy_iterator():\n",
    "            label=label[0]\n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "        return images,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zufU32IgdHIj"
   },
   "outputs": [],
   "source": [
    "def runResNet(data, epochs, batch_size, shape = [227, 227, 3], learn_rate = 0.0001, early_stopping_patience = 20, testsize=0.2, kfold=5):\n",
    "\n",
    "    num_samples = len(data) # number of sample images\n",
    "    num_folds = kfold   #number of folds specified\n",
    "\n",
    "    test = data.take(int(testsize*num_samples)) # testdataset\n",
    "    train = data.skip(int(testsize*num_samples)) # training dataset including validation data\n",
    "    \n",
    "    foldsize = int(len(train)/num_folds)  # number of images in one fold\n",
    "\n",
    "    #Get size for printing\n",
    "    train_size = tf.data.experimental.cardinality(train).numpy()\n",
    "    test_size = tf.data.experimental.cardinality(test).numpy()\n",
    "    #validation_size = tf.data.experimental.cardinality(validation).numpy()\n",
    "    print(\"Training data size:\", int(train_size *(kfold-1)/kfold))\n",
    "    print(\"Test data size:\", test_size)\n",
    "    print(\"Validation data size:\", int(train_size /kfold))\n",
    "    \n",
    "    test = test.shuffle(buffer_size = test_size, seed = 13) #shuffles the testdataset\n",
    "    test = test.batch(batch_size=batch_size, drop_remainder=True)   #batch the testdataset\n",
    "\n",
    "    # iterate over folds\n",
    "    for fold in range(num_folds):\n",
    "\n",
    "      print('Fold number', fold+1)\n",
    "\n",
    "      model = resnet50(shape) # build model\n",
    "      model.compile(loss='binary_crossentropy', optimizer=tf.optimizers.SGD(lr=learn_rate), metrics=['accuracy']) # compile model\n",
    "      #model.summary()\n",
    "\n",
    "      # split training data in training and validation data for current fold\n",
    "      valfold = train.take(foldsize*(fold+1))\n",
    "      valfold = valfold.skip(foldsize*fold)\n",
    "      trainfold1 = train.take(foldsize*fold)\n",
    "      trainfold2 = train.skip(foldsize*(fold+1))\n",
    "      trainfold = trainfold1.concatenate(trainfold2)\n",
    "\n",
    "      #Shuffle data  in folds\n",
    "      trainfold = trainfold.shuffle(buffer_size = train_size, seed = 13)\n",
    "      valfold = valfold.shuffle(buffer_size = train_size, seed = 13)\n",
    "      \n",
    "      # batch data\n",
    "      trainfold = trainfold.batch(batch_size=batch_size, drop_remainder=True)\n",
    "      valfold = valfold.batch(batch_size=batch_size, drop_remainder=True)\n",
    "\n",
    "      #Early stopping callback\n",
    "      earlystopper = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=early_stopping_patience, verbose=1,mode='min') # Early stopping function to prevent overfitting\n",
    "\n",
    "      #Train network\n",
    "      model.fit(trainfold,\n",
    "                epochs=epochs,\n",
    "                validation_data=valfold,\n",
    "                validation_freq=1, callbacks = [earlystopper])\n",
    "\n",
    "      #Evaluate test data\n",
    "      model.evaluate(test)\n",
    "  \n",
    "    return model, test, train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MQyDRaUGzgb"
   },
   "source": [
    "# Running the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ot4RoYJTp4I9"
   },
   "outputs": [],
   "source": [
    "def run_preprocessing(input_dir, x_shape = 227, y_shape = 227,\n",
    "                      data_standardization_switch = False, grayscale_switch = False, edge_detection_switch = False,\n",
    "                      rotate_180_switch = False, mirroring_switch = False, zoom_switch = False,\n",
    "                      rotate_180_rate = 1, mirroring_rate = 1,\n",
    "                      zoom_rate = 1, zoom_factor = 2, test_predict = False):\n",
    "    \n",
    "    #Geting a list of all files in the input directory\n",
    "    filenames = get_file_list(input_dir) \n",
    "    \n",
    "    #Initializing the dataset\n",
    "    dataset = False\n",
    "\n",
    "    for f in range(len(filenames)):\n",
    "        \n",
    "        #Printing the progress while running the preprocessing\n",
    "        print_preprocessing_progress(f, len(filenames))\n",
    "        \n",
    "        #Loading the current file\n",
    "        image = open_image(filenames[f])\n",
    "        \n",
    "        #Computing the label (is a stemcell (1) or not (0))\n",
    "        label = compute_is_stemcell(filenames[f])\n",
    "\n",
    "        #Preprocessing the image\n",
    "        image = data_preprocessing(image, data_standardization_switch, grayscale_switch, edge_detection_switch)\n",
    "\n",
    "        #Applying the data augmentation methods\n",
    "        image, label, augmented = data_augmentation(image, label, f/len(filenames), rotate_180_switch, \n",
    "                                                    mirroring_switch, zoom_switch, rotate_180_rate, \n",
    "                                                    mirroring_rate, zoom_rate, zoom_factor)\n",
    "\n",
    "        #Resizing the data to the requested shape\n",
    "        image = resize_image(image, x_shape, y_shape)\n",
    "\n",
    "        #Get number of augmentations + original picture \n",
    "        aug_size = int(tf.shape(image)[0])\n",
    "            \n",
    "        #Concatenating the input data to a tf.ConcatenateDataset\n",
    "        dataset = concatenate_data(dataset, image, label, augmented)  \n",
    "        \n",
    "# -------------------------------------------------------------------------------------------------------- #\n",
    "        \n",
    "    if test_predict:\n",
    "        #Geting a list of all testing files in the input directory\n",
    "        filenames_test_predict = get_file_list(input_dir, test_predict = test_predict)\n",
    "        \n",
    "        #Initializing the dataset\n",
    "        dataset_test_predict = False\n",
    "        \n",
    "        for f in range(len(filenames_test_predict)):\n",
    "        \n",
    "            #Printing the progress while running the preprocessing\n",
    "            print_preprocessing_progress(f, len(filenames_test_predict))\n",
    "\n",
    "            #Loading the current file\n",
    "            image = open_image(filenames[f])\n",
    "\n",
    "            #Preprocessing the image\n",
    "            image = data_preprocessing(image, data_standardization_switch, grayscale_switch, edge_detection_switch)\n",
    "\n",
    "            #Resizing the data to the requested shape\n",
    "            image = resize_image(image, x_shape, y_shape)\n",
    "\n",
    "            #Concatenating the input data to a tf.ConcatenateDataset\n",
    "            dataset_test_predict = concatenate_data(dataset_test_predict, image, test_predict = test_predict)  \n",
    "    \n",
    "        return dataset, aug_size, dataset_test_predict\n",
    "    \n",
    "# -------------------------------------------------------------------------------------------------------- #\n",
    "    \n",
    "    else:\n",
    "        return dataset, aug_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 558612,
     "status": "ok",
     "timestamp": 1611649071067,
     "user": {
      "displayName": "Lalo Massieu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgQI8qRGMuzGoZtMw8EQBGGyC7dbuX1S77nkKai=s64",
      "userId": "09968052887058816350"
     },
     "user_tz": -60
    },
    "id": "K6KI-hujp9fg",
    "outputId": "c5d68c22-c5f8-4128-f7d9-85d8f8ccaa24"
   },
   "outputs": [],
   "source": [
    "input_dir = '/content/drive/MyDrive/Colab Notebooks/Pictures_Project_B/'\n",
    "data, aug_size = run_preprocessing(input_dir, x_shape = 227, y_shape = 227,\n",
    "                      data_standardization_switch = True, grayscale_switch = False, edge_detection_switch = False,\n",
    "                      rotate_180_switch = True, mirroring_switch = False, zoom_switch = False,\n",
    "                      rotate_180_rate = 1, mirroring_rate = 1,\n",
    "                      zoom_rate = 1, zoom_factor = 2, test_predict = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g5XGeESrqdg_"
   },
   "outputs": [],
   "source": [
    "AlexNet(data, epochs = 30, batch_size = 10,  train_split_rate = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 198193,
     "status": "ok",
     "timestamp": 1611650999986,
     "user": {
      "displayName": "Lalo Massieu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgQI8qRGMuzGoZtMw8EQBGGyC7dbuX1S77nkKai=s64",
      "userId": "09968052887058816350"
     },
     "user_tz": -60
    },
    "id": "9EsBsDY2dSmc",
    "outputId": "b3a0b113-0627-4346-e381-a8c799a57309"
   },
   "outputs": [],
   "source": [
    "model,test,train = runResNet(data, epochs = 30, batch_size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cRu0YkP24kym"
   },
   "outputs": [],
   "source": [
    "model, test, train = runNet(data, aug_size, data_test_predict=[], epochs=100, batch_size=10, testsize=0.15, kfold=5, learn_rate=0.001, early_stopping_patience=20, shape=[227,227,3], ResNet=True, AlexNet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 184400,
     "status": "ok",
     "timestamp": 1611651000656,
     "user": {
      "displayName": "Lalo Massieu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgQI8qRGMuzGoZtMw8EQBGGyC7dbuX1S77nkKai=s64",
      "userId": "09968052887058816350"
     },
     "user_tz": -60
    },
    "id": "2WyYGIzj_i1U",
    "outputId": "3e69b1d6-5816-4c1b-c936-4731142a2409"
   },
   "outputs": [],
   "source": [
    "save_model(model, suffix=\"Resnet-Full_Dataset-NoAug20-3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4528,
     "status": "ok",
     "timestamp": 1611651650175,
     "user": {
      "displayName": "Lalo Massieu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgQI8qRGMuzGoZtMw8EQBGGyC7dbuX1S77nkKai=s64",
      "userId": "09968052887058816350"
     },
     "user_tz": -60
    },
    "id": "nd8zWfTim5OS",
    "outputId": "d04c1516-e7ad-49b0-d9e0-f6b664ce8cba"
   },
   "outputs": [],
   "source": [
    "full_set=load_model(\"drive/My Drive/Data/models/20210126-08491611650999-Resnet-Full_Dataset-NoAug20-3.h5\")\n",
    "predictions=full_set.predict(test,batch_size=40, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3109,
     "status": "ok",
     "timestamp": 1611651601029,
     "user": {
      "displayName": "Lalo Massieu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgQI8qRGMuzGoZtMw8EQBGGyC7dbuX1S77nkKai=s64",
      "userId": "09968052887058816350"
     },
     "user_tz": -60
    },
    "id": "qHxtkeJjxsLf",
    "outputId": "4e7da7ab-5237-40cb-888a-2c1af4927536"
   },
   "outputs": [],
   "source": [
    "full_set.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w_aqfsc4tSSx"
   },
   "outputs": [],
   "source": [
    "xtest,ytest= unbatchify(test,test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SKy1jTtZ4RG3"
   },
   "outputs": [],
   "source": [
    "xtest=np.asarray(xtest)\n",
    "ytest=np.asarray(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c19XPkEIunLM"
   },
   "outputs": [],
   "source": [
    "predictions=tf.squeeze(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VqMbbo8HuAdK"
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame({\"Pred\":predictions,\"Labels\":ytest})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_f-cqz_vz9VM"
   },
   "outputs": [],
   "source": [
    "df['Pred'] = np.where((df['Pred'] >= 0.5),1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8M5BZGQZv-9i"
   },
   "outputs": [],
   "source": [
    "df['Result'] = np.where((df['Pred'] == df['Labels']),1, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cV-HCbjN5l-y"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 544,
     "status": "ok",
     "timestamp": 1611651667625,
     "user": {
      "displayName": "Lalo Massieu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgQI8qRGMuzGoZtMw8EQBGGyC7dbuX1S77nkKai=s64",
      "userId": "09968052887058816350"
     },
     "user_tz": -60
    },
    "id": "nt7y3ge9w1_x",
    "outputId": "3bb45590-7e63-40a4-d6c2-bf6b1becc76f"
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CYles0f2mDwB"
   },
   "outputs": [],
   "source": [
    "def create_dataset(input_dir):\n",
    "    \n",
    "    filenames = get_file_list(input_dir)\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    for file in filenames:\n",
    "    \n",
    "        counter += 1\n",
    "        print(counter,'von',len(filenames))\n",
    "        \n",
    "        image = np.array(Image.open(file))\n",
    "        \n",
    "        #image = preprocess(image)\n",
    "        #image = tf.image.rgb_to_grayscale(image)\n",
    "        \n",
    "        if 'Stemcell' in file:\n",
    "            label = np.array([1])\n",
    "        else:\n",
    "            label = np.array([0])\n",
    "        \n",
    "        #print(image)\n",
    "        #print(image.shape)\n",
    "        #print(label.shape)\n",
    "        data = tf.data.Dataset.from_tensors((image,label))\n",
    "        \n",
    "        if counter == 1:\n",
    "            dataset = data\n",
    "        else:\n",
    "            dataset = dataset.concatenate(data)\n",
    "            #dataset = tf.data.Dataset.zip((dataset,data))\n",
    "\n",
    "    #dataset = tf.data.TFRecordDataset(filenames)\n",
    "    \n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SVHFIhrrhU2h"
   },
   "outputs": [],
   "source": [
    "def create_dataset_s(input_dir):\n",
    "    \n",
    "    filenames = get_file_list(input_dir)\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    for file in filenames:\n",
    "    \n",
    "        counter += 1\n",
    "        print(counter,'von',len(filenames))\n",
    "        \n",
    "        image = np.array(Image.open(file))\n",
    "        \n",
    "        image = preprocessing(image)\n",
    "        #tf.image.rgb_to_grayscale(image)\n",
    "        \n",
    "        if 'Stemcell' in file:\n",
    "            label = np.array([1])\n",
    "        else:\n",
    "            label = np.array([0])\n",
    "        \n",
    "        #print(image)\n",
    "        #print(image.shape)\n",
    "        #print(label.shape)\n",
    "        #image, label = data_augmentation180(image,label)\n",
    "        image, label = slice_into_x_y(image, label, x = 227, y = 227)\n",
    "        \n",
    "        data = tf.data.Dataset.from_tensor_slices((image,label))\n",
    "        \n",
    "        if counter == 1:\n",
    "            dataset = data\n",
    "        else:\n",
    "            dataset = dataset.concatenate(data)\n",
    "            #dataset = tf.data.Dataset.zip((dataset,data))\n",
    "\n",
    "    #dataset = tf.data.TFRecordDataset(filenames)\n",
    "    \n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 168804,
     "status": "ok",
     "timestamp": 1611313860389,
     "user": {
      "displayName": "Lalo Massieu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgQI8qRGMuzGoZtMw8EQBGGyC7dbuX1S77nkKai=s64",
      "userId": "09968052887058816350"
     },
     "user_tz": -60
    },
    "id": "9yWPlu7KQTgC",
    "outputId": "c94d0dc3-b541-4335-bfe6-0fd86e3a8896"
   },
   "outputs": [],
   "source": [
    "create_dataset_s(input_dir=\"/content/gdrive/MyDrive/Colab Notebooks/Pictures_Project_B/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 130213,
     "status": "ok",
     "timestamp": 1611314192466,
     "user": {
      "displayName": "Lalo Massieu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgQI8qRGMuzGoZtMw8EQBGGyC7dbuX1S77nkKai=s64",
      "userId": "09968052887058816350"
     },
     "user_tz": -60
    },
    "id": "Dlf_oFq0Pr8C",
    "outputId": "eada0fd9-b25c-46d3-9bd6-de94f9f6a18d"
   },
   "outputs": [],
   "source": [
    "input_dir = '/content/gdrive/MyDrive/Colab Notebooks/Pictures_Project_B/'\n",
    "\n",
    "datadir = input_dir + 'dataset/'\n",
    "\n",
    "data = create_dataset_s(input_dir)\n",
    "\n",
    "tf.data.experimental.save(data,datadir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LrTdeKIbSk2U"
   },
   "outputs": [],
   "source": [
    "# for loading the dataset after saving it\n",
    "# you might have to adjust the shape depending on how you slice the data\n",
    "elspec = (tf.TensorSpec(shape=(1920, 2560, 3), dtype=tf.uint8, name=None),tf.TensorSpec(shape=(1,), dtype=tf.int32, name=None))\n",
    "data = tf.data.experimental.load(datadir, elspec)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Project_B.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
